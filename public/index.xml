<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://phatcvo.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Nov 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://phatcvo.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[MPC] Linear Parameter-Varying - Model Predictive Control for Racing</title>
      <link>https://phatcvo.github.io/blog/lpv-mpp-mcp-for-racing/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/lpv-mpp-mcp-for-racing/</guid>
      <description>Planning and control for autonomous racing vehicles. This project allows you to solve the autonomous racing driving problem using advanced control theory. Particularly, here it is presented a collaborative work using optimal strategies. The Model Predictive Control (MPC) strategy is used online for computing the optimal trajectory maximizing vehicle velocity but also for computing the optimal control actions that make the vehicle to follow the computed references. All the algorithms are solved in real time employing the Operator Splitting Quadratic Program (OSQP) solver.</description>
    </item>
    <item>
      <title>[MPC] Learning Model Predictive Control for autonomous racing</title>
      <link>https://phatcvo.github.io/blog/learning-mpc-for-racing/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/learning-mpc-for-racing/</guid>
      <description>The Learning Model Predictive Control is a data-driven control framework developed at UCB in the MPC lab. In this example, we implemented the LMPC for the autonomous racing problem. The controller drives several laps on race track and it learns from experience how to drive faster.
In the above animation we see the vehicle&amp;rsquo;s closed-loop trajectory (in black) for laps 5, 30, 31 and 32. At each time instant the LMPC leverages forecast to plan the vehicle trajectory (in red) few seconds into the future.</description>
    </item>
    <item>
      <title>[MPC] Trajectory tracking with a quadrotor by LMPC</title>
      <link>https://phatcvo.github.io/blog/mpc-for-quadrotor/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/mpc-for-quadrotor/</guid>
      <description>The non-linear dynamics are linearized about a linearization trajectory $\bar{x}, \bar{u}$ while the quadrotor aims to track a different reference trajectory $x_{ref}, u_{ref}$.
The MPC controller solves the following optimization problem :
$$\begin{aligned} \min_{x_{1:N},u_{1:N-1}} \quad &amp;amp; \sum_{i=1}^{N-1} \bigg[ (x_i - x_{i,ref})^TQ(x_i - x_{i,ref}) + (u_i - u_{i,ref})^TR(u_i - u_{i,ref}) \bigg] + \frac{1}{2} (x_N - x_{N,ref})^TQ_f(x_N - x_{N,ref}) \\ \textrm{s.t.} \quad &amp;amp; x_1 = x_{\text{IC}} \\ &amp;amp; x_{k+1} = f(\bar{x}_k, \bar{u}_k) + \bigg[\frac{\partial f}{\partial x} \bigg|_{\bar{x}_k, \bar{u}_k} \bigg](x_k - \bar{x}_k) + \bigg[\frac{\partial f}{\partial u} \bigg|_{\bar{x}_k, \bar{u}_k} \bigg](u_k - \bar{u}_k) \quad \text{for } i = 1,2,\ldots,N-1 \\ &amp;amp; u_{min} \leq u_i \leq u_{max} \quad ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \text{for } i = 1,2,\ldots,N-1 \\ \end{aligned}$$</description>
    </item>
    <item>
      <title>[MPC] Model Predictive Control with discrete-time Control Barrier Functions</title>
      <link>https://phatcvo.github.io/blog/model-predictive-control-with-discrete-time-control-barrier-functions/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/model-predictive-control-with-discrete-time-control-barrier-functions/</guid>
      <description>The MPC-CBF optimization problem is given by:
$$\begin{aligned} \min_{u_{t: t+N-1 \mid t}} \quad &amp;amp; \frac{1}{2} \tilde{x}_N^T Q_x \tilde{x}_N+\sum_{k=0}^{N-1} \frac{1}{2} \tilde{x}_k^T Q_x \tilde{x}_k+\frac{1}{2} u_k^T Q_u u_k\\ \textrm{s.t.} \quad &amp;amp; x_{t+k+1 \mid t}=x_{t+k \mid t}+f\left(x_{t+k \mid t}, u_{t+k \mid t}\right) \cdot T_s, \quad k=0, . ., N-1,\\ &amp;amp; x_{\min } \leq x_{t+k \mid t} \leq x_{\max }, \quad k=0, \ldots, N-1,\\ &amp;amp; u_{\min } \leq u_{t+k \mid t} \leq u_{\max }, \quad k=0, \ldots, N-1, \\ &amp;amp; x_{t \mid t}=x_t, \\ &amp;amp; \Delta h\left(x_{t+k \mid t}, u_{t+k \mid t}\right) \geq-\gamma h\left(x_{t+k \mid t}\right), \quad k=0, \ldots, N-1 \\ \end{aligned}$$</description>
    </item>
    <item>
      <title>[LQR] iLQR/DDP algorithm for Non-linear trajectory optimization</title>
      <link>https://phatcvo.github.io/blog/iterative-linear-quadratic-regulator-and-differential-dynamic-programming/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/iterative-linear-quadratic-regulator-and-differential-dynamic-programming/</guid>
      <description>Differential Dynamic Programming (DDP) is an indirect method which optimizes over the unconstrained control-space. It uses a 2nd-order Taylor series approximation of the cost-to-go in Dynamic Programming (DP) to compute Newton steps on the control trajectory.
iLQR: Only keeps the first-order terms (Gauss-Newton approximation), which is similar to Riccati iterations, but accounts for the regularization and line-search required to handle the nonlinearity. DDP: Second-order terms included (Newton approximation). The iLQR/DDP controller solves the following finite-horizon optimization (Non-linear trajectory optimization) problem:</description>
    </item>
    <item>
      <title>[MPC] Nominal Nonlinear Model Predictive Control</title>
      <link>https://phatcvo.github.io/blog/nominal-nonlinear-model-predictive-control/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/nominal-nonlinear-model-predictive-control/</guid>
      <description>Nonlinear model predictive control (NMPC) is a popular control method for multivariable control problems with important process constraints. The dynamic equation system is assumed to be given by differential algebraic equations (DAE). The code is mostly meant to be used as a way to verify the performance of more novel algorithms against an implementation more likely to be found in industry. It has the following features:
Cheap NMPC implementation for both receding and shrinking time horizons Parameter and state estimation using the UKF Efficient solution of nonlinear dynamic optimization formulation using automatic differentiation Always feasible due to soft-constraints Example This is a basic nonlinear model predictive control (NMPC) implementation in Python with soft constraints, which uses an Unscented Kalman filter for state estimation.</description>
    </item>
    <item>
      <title>[MPC] Linear Model Predictive Control</title>
      <link>https://phatcvo.github.io/blog/linear-model-predictive-control/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/linear-model-predictive-control/</guid>
      <description>Vehicle model linearization Vehicle model is
$$ \dot{x} = vcos(\phi)$$
$$ \dot{y} = vsin((\phi)$$
$$ \dot{v} = a$$
$$ \dot{\phi} = \frac{vtan(\delta)}{L}$$
State and Input vector:
$$ z = \begin{bmatrix} x-position \\\ y-position \\\ velocity \\\ yaw-angle\end{bmatrix} = \begin{bmatrix} x \\\ y \\\ v \\\ \phi\end{bmatrix} $$
$$u = \begin{bmatrix} acceleration \\ steering-angle \end{bmatrix} = \begin{bmatrix} a \\ \delta \end{bmatrix} $$
ODE is
$$ \dot{z} =\frac{\partial }{\partial z} z = f(z, u) = A&amp;rsquo;z+B&amp;rsquo;u$$</description>
    </item>
    <item>
      <title>[RL] DRL-Basics Reinforcement Learning</title>
      <link>https://phatcvo.github.io/blog/drl-basicsrl/</link>
      <pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/drl-basicsrl/</guid>
      <description>Introduction The goal of this document is to keep track the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations and covers different classes of deep RL methods, value-based or policy-based, model-free or model-based, etc.
Different classes of deep RL methods can be identified. This document will focus on the following ones:
Value-based algorithms (DQN&amp;hellip;) used mostly for discrete problems like video games.</description>
    </item>
    <item>
      <title>[Survey] Safe-Optimal Control for Motional Planning based on RL</title>
      <link>https://phatcvo.github.io/blog/safe-optimal-control-for-motional-planning-based-on-rl/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/safe-optimal-control-for-motional-planning-based-on-rl/</guid>
      <description>Table of contents Optimal Control Dynamic Programming Linear Programming Tree-Based Planning Control Theory Model Predictive Control Safe Control Robust Control Risk-Averse Control Value-Constrained Control State-Constrained Control and Stability Uncertain Dynamical Systems Game Theory Sequential Learning Multi-Armed Bandit Best Arm Identification Black-box Optimization Reinforcement Learning Theory Value-based Policy-based Policy Gradient Actor-critic Derivative-free Model-based Exploration Hierarchy and Temporal Abstraction Partial Observability Transfer Multi-agent Representation Learning Offline Learning from Demonstrations Imitation Learning Applications to Autonomous Driving Inverse Reinforcement Learning Applications to Autonomous Driving Motion Planning Search Sampling Optimization Reactive Architecture and applications Optimal Control: Dynamic Programming (book) Dynamic Programming, Bellman R.</description>
    </item>
    <item>
      <title>[Survey] Survey of Motion Planning strategies</title>
      <link>https://phatcvo.github.io/blog/survey-of-motion-planning-strategies/</link>
      <pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://phatcvo.github.io/blog/survey-of-motion-planning-strategies/</guid>
      <description>Search and Heuristic A Note on Two Problems in Connexion with Graphs by Dijkstra A Formal Basis for the Heuristic Determination of Minimum Cost Paths by Hart On the complexity of admissible search algorithms by Martelli Heuristic Search Viewed as Path Finding in a Graph by Pohl R* Search by Likhachev abd Stentz Incremental A* by Koenig and Likhachev Lifelong planning A* by Koenig, Likhachev and Furcy Real-Time Adaptive A* by Koenig and Likhachev Multiheuristic A* by Aine, Swaminathan, Narayan, Hwang and Likhachev Linear Space Best-First Search Replanning Optimal and efficient path planning for partially-known environments by Stentz The Focussed D* Algorithm for Real-Time Replanning by Stentz Fast Replanning for Navigation in Unknown Terrain by Koenig and Likhachev Using Interpolation to Improve Path Planning: The Field D* Algorithm by Ferguson and Stentz Replanning with RRTs by Ferguson, Kalra and Stentz RRTX: Real-Time Motion Planning/Replanning for Environments with Unpredictable Obstacles by Otte and Frazzoli Real-time planning ARA*: Anytime A* with Provable Bounds on Sub-Optimality by Likhachev, Gordon and Thrun Anytime Dynamic A*: An Anytime, Replanning Algorithm by Likhachev, Ferguson, Gordon, Stentz and Thrun Anytime search in dynamic graphs by Likhachev, Ferguson, Gordon, Stentz and Thrun Anytime RRTs by Ferguson and Stentz RRTs Rapidly Exploring Random Trees: A new tool for path planning by Lavelle RRT-connect: An efficient approach to single-query path planning by Kuffner and Lavalle Sampling-based Algorithms for Optimal Motion Planning by Karaman and Frazolli Informed RRT*: Optimal Sampling-based Path Planning Focused via Direct Sampling of an Admissible Ellipsoidal Heuristic by Gammell, Srinivasa and Barfoot Batch Informed Trees (BIT*): Sampling-based Optimal Planning via the Heuristically Guided Search of Implicit Random Geometric Graphs by Gammell, Srinivasa and Barfoot Sampling-Based Path Planning on Configuration-Space Costmaps by Jaillet, Cortes and Simeon Addressing Cost-Space Chasms in Manipulation Planning by Berenson, Simeon and Srinivasa Optimal Kinodynamic Motion Planning using Incremental Sampling-based Methods by Karaman and Frazzoli Sampling-based Optimal Motion Planning for Non-holonomic Dynamical Systems by Karaman and Frazzoli State lattice-based planners Efficient constrained path planning via search in state lattices by Pivtoraiko and Kelly Differentially Constrained Mobile Robot Motion Planning in State Lattices by Pivtoraiko, Knepper and Kelly Kinodynamic motion planning with state lattice motion primitives by Pivtoraiko and Kelly Trajectory optimization The dynamic window approach to collision avoidance by Fox, Burgard and Thrun CHOMP: Covariant Hamiltonian optimization for motion planning by Zucker et al Timed elastic bands, Optimizing TEBs using sparse model, TEBs with distinctive topologies, TEBs for car-like robots by Rösmann, Hoffmann and Bertram Trajectory generation for car-like robots On curves of minimal length with a constraint on average curvature, and with prescribed initial and terminal positions and tangents by Dubins Optimal paths for a car that goes both forwards and backwards by Reeds and Shepp Optimal Trajectories for Nonholonomic Mobile Robots by Soueres and Boissonnat Shortest paths synthesis for a car-like robot by Soueres and Laumond Smooth Local Path Planning for Autonomous Vehicles by Kanayama and Hartman Trajectory Generation for Car-like robots using cubic curvature polynomials by Nagy and Kelly Optimal Rough Terrain Trajectory Generation for Wheeled Mobile Robots by Howard and Kelly Planning for mobile robots A Guide to Heuristic-based Path Planning by Ferguson, Likhachev and Stentz Planning continuous-curvature paths for car-like robots by Scheuer and Fraichard Real Time Continuous Curvature Path Planner for an Autonomous Vehicle in an Urban Environment by Knowles Planning Long Dynamically Feasible Maneuvers for Autonomous Vehicles by Likhachev and Ferguson Autonomous Driving in Structured and Unstructured Environments by Kolski, Ferguson, Bellino and Siegwart Toward Reliable Off Road Autonomous Vehicles Operating in Challenging Environments by Kelly et al.</description>
    </item>
  </channel>
</rss>
