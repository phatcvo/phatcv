{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6bQDnCwVec8"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Install drake (and underactuated).\n",
    "if 'google.colab' in sys.modules and importlib.util.find_spec('underactuated') is None:\n",
    "    urlretrieve(f\"http://underactuated.csail.mit.edu/scripts/setup/setup_underactuated_colab.py\",\n",
    "                \"setup_underactuated_colab.py\")\n",
    "    from setup_underactuated_colab import setup_underactuated\n",
    "    setup_underactuated(underactuated_sha='560c2adace05eb20ebd78377582015d5b2d3859a', drake_version='0.25.0', drake_build='releases')\n",
    "\n",
    "from IPython import get_ipython\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import meshcat\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "from pydrake.all import DiagramBuilder, LinearSystem, VectorSystem, Simulator, WrapToSystem, DiscreteAlgebraicRiccatiEquation\n",
    "from pydrake.systems.controllers import (DynamicProgrammingOptions,\n",
    "                                         FittedValueIteration, PeriodicBoundaryCondition)\n",
    "from pydrake.systems.pyplot_visualizer import PyPlotVisualizer\n",
    "from pydrake.systems.framework import Context, PortDataType\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import underactuated\n",
    "from underactuated.double_integrator import DoubleIntegratorVisualizer\n",
    "from underactuated.jupyter import AdvanceToAndVisualize, SetupMatplotlibBackend, running_as_notebook\n",
    "from underactuated.meshcat_utils import plot_surface\n",
    "\n",
    "plt_is_interactive = SetupMatplotlibBackend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYTV8z5cAI4J"
   },
   "source": [
    "## Problem Description\n",
    "In this problem we will be implementing the fitted value iteration algorithm and using it to solve the quadratic cost double integrator. While Drake provides an easy to use implementation of fitted value iteration, here we will be using a neural net to approximate the cost-to-go function instead. We will then use our cost-to-go function approximation to generate a control policy. \n",
    "\n",
    "**These are the main steps of the notebook:**\n",
    "1. Implement the neural network in order to approximate the cost-to-go function.\n",
    "2. Write the target network update in the training loop.\n",
    "3. Extract the policy from the cost-to-go function estimate.\n",
    "4. Answer the written questions at the bottom and submit them to Gradescope as a .pdf file.\n",
    "\n",
    "Note that in order to pass the autograder, you do not need to tune any hyperparameters! This includes random seeds, learning rate, training iterations, and initializations. Feel free to play around with these, but set them to the original values when you want to test with the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DraZ3Vwa5py4"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "  np.random.seed(seed)\n",
    "  torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auHY36I7B01H"
   },
   "source": [
    "## Defining our Cost Function Approximator\n",
    "\n",
    "In order to perform value iteration in a continuous state space, we will use a neural network to approximate the cost-to-go $J(x)$. Our network will take as input the double integrator state $x = [q, \\dot q]$, and output $J(x)$. Implement the following architecture:\n",
    "\n",
    "- Linear layer with 2 inputs, 120 outputs\n",
    "- Leaky relu nonlinearity\n",
    "- Linear layer with 120 inputs, 84 outputs\n",
    "- Leaky relu nonlinearity\n",
    "- Linear layer with 84 inputs, 1 ouput\n",
    "\n",
    "An example placeholder layer is implemented for you, and we show you how to call it functionally in `forward` with a leaky relu nonlinearity. This is an alternative way to define a network instead of stacking layers using `nn.Sequential`. Try it out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxgwSvEC5-qa"
   },
   "outputs": [],
   "source": [
    "# Define the function approximator for J\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ### TODO ###\n",
    "        self.fc1 = nn.Linear(2, 1) # placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO ###\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJqf-kNwKqSd"
   },
   "source": [
    "## Defining the state-space and cost function\n",
    "\n",
    "Below we define two costs. We have the quadratic regulator cost:\n",
    "\n",
    "$$g(x, u) = x^T Q x + u^T R u$$\n",
    "\n",
    "And we have the minimum time cost:\n",
    "\n",
    "$$g(x) = 1 \\text{  if  } x \\neq 0, \\text{  else  } 0$$\n",
    "\n",
    "These will both seek to stabilize the system to the origin $x = 0$. The solution to the quadratic regulator cost makes use of drake to generate to solve the algebraic riccati equation and generate the solution matrix $S$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uja0KdGxC_4F"
   },
   "outputs": [],
   "source": [
    "Q = torch.eye(2)\n",
    "R = torch.eye(1)\n",
    "def quadratic_regulator_cost(xt, ut):\n",
    "    return xt.matmul(Q.matmul(xt.transpose(-2,-1))) + ut.matmul(R.matmul(ut.transpose(-2,-1)))\n",
    "\n",
    "def quadratic_regulator_solution(xt, timestep):\n",
    "  S = DiscreteAlgebraicRiccatiEquation(A=(np.eye(2)+timestep*A.numpy()),\n",
    "                                       B=timestep*B.numpy(),\n",
    "                                       Q=Q, R=R)\n",
    "  return xt.matmul(torch.from_numpy(S).float().matmul(xt.transpose(-2,-1)))\n",
    "\n",
    "# Define the cost function\n",
    "def min_time_cost(xt):\n",
    "    at_goal = torch.isclose(xt, torch.zeros(1,2), atol=1e-3)\n",
    "    # cost = 1 if ~at_goal * [1;1] >= 1, 0 otherwise.\n",
    "    return torch.min((~at_goal).float().matmul(torch.ones(2,1)), torch.ones(1))\n",
    "\n",
    "def min_time_solution(xt):\n",
    "  # Caveat: this does not take the time discretization (zero-order hold on u) into account.\n",
    "  q = xt[:,:,0]\n",
    "  qdot = xt[:,:,1]\n",
    "  # mask indicates that we are in the regime where u = +1.\n",
    "  mask = ((qdot < 0) & (2*q <= qdot.pow(2))) | ((qdot >= 0) & (2*q < -qdot.pow(2)))\n",
    "  T = torch.empty(q.size())\n",
    "  T[mask] = 2*(.5*(qdot[mask].pow(2)) - q[mask]).sqrt() - qdot[mask]\n",
    "  T[~mask] = qdot[~mask] + 2*(.5*(qdot[~mask].pow(2)) + q[~mask]).sqrt()\n",
    "  return T.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo2JYrug9UHS"
   },
   "source": [
    "In the cell below, we set the state space samples we will use to train the network, the action space discretization, and the timestep increment. We produce a mesh over state space, and for each state in this mesh we produce the cost $g(x, u)$ and the next state for each action taken. Because the double integrator is a linear system, generating the next states can be done very simply using the coefficient matrices $A$ and $B$! There is also a method below that we will use to plot the estimated cost surface produced by our neural net, and plot the analytical cost surface. Feel free to take a look if you're interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klEO8sBoBjuq"
   },
   "outputs": [],
   "source": [
    "# Define state-space and timestep increment, and cost type\n",
    "num_x1s = 31\n",
    "num_x2s = 51\n",
    "num_states = num_x1s*num_x2s\n",
    "num_actions = 9\n",
    "x1s = torch.linspace(-3,3,num_x1s)\n",
    "x2s = torch.linspace(-3,3,num_x2s)\n",
    "us = torch.linspace(-1,1,num_actions)\n",
    "timestep = 0.1\n",
    "\n",
    "# Define batch of states with actions\n",
    "X1s, X2s = torch.meshgrid(x1s, x2s)\n",
    "X = torch.stack((X1s.flatten(), X2s.flatten()), 1).unsqueeze(1)\n",
    "X1s_wU, X2s_wU, Us_wX = torch.meshgrid(x1s, x2s, us)\n",
    "XwithU = torch.stack((X1s_wU.flatten(0,1), X2s_wU.flatten(0,1)), 2).unsqueeze(2)\n",
    "UwithX = Us_wX.flatten(0,1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "# Define coefficient matrices for double integrator state-space representation\n",
    "A = torch.tensor([[0., 1.], [0., 0.]])\n",
    "B = torch.tensor([[0.], [1.]])\n",
    "At = A.transpose(0, 1)\n",
    "Bt = B.transpose(0, 1)\n",
    "\n",
    "# Generate next states and costs\n",
    "Xnext = XwithU + timestep * (XwithU.matmul(At) + UwithX.matmul(Bt))\n",
    "G_quadratic = timestep*quadratic_regulator_cost(XwithU, UwithX)\n",
    "G_min_time = timestep*min_time_cost(XwithU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozYW88FGD8FQ"
   },
   "outputs": [],
   "source": [
    "def plot_and_compare(net, timestep, min_time=False):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    J = net.forward(X)\n",
    "\n",
    "  # Plot estimated cost surface\n",
    "  fig = plt.figure(figsize=(9, 4))\n",
    "  ax1, ax2 = fig.subplots(1, 2, subplot_kw=dict(projection='3d'))\n",
    "  ax1.set_xlabel(\"q\")\n",
    "  ax1.set_ylabel(\"qdot\")\n",
    "  ax1.set_title(\"Estimated Cost-to-Go\")\n",
    "  ax1.plot_surface(X1s, X2s, J.view(X1s.size()).detach().numpy(), rstride=1, cstride=1, cmap=cm.jet)\n",
    "  \n",
    "  # Plot analytical cost surface\n",
    "  if min_time:\n",
    "    Jd = min_time_solution(X)\n",
    "  else:\n",
    "    Jd = quadratic_regulator_solution(X, timestep)\n",
    "  ax2.set_xlabel(\"q\")\n",
    "  ax2.set_ylabel(\"qdot\")\n",
    "  ax2.set_title(\"Analytical Cost-to-Go\")\n",
    "  ax2.plot_surface(X1s, X2s, Jd.view(X1s.size()).detach().numpy(), rstride=1, cstride=1, cmap=cm.jet)\n",
    "    \n",
    "  # Score is worst absolute different (e.g. infinity-norm) of the samples\n",
    "  criterion = nn.MSELoss()\n",
    "  score = criterion(J, Jd).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWxDj23hMT5m"
   },
   "source": [
    "## Fitting the Cost-to-go Function\n",
    "\n",
    "We can now fit the cost-to-go function. The idea is to train a single neural network to approximate the cost-to-go function, and use a target network to represent our \"latest cost-to-go function estimate\". In the `solve` method below, write one line of code that takes the weights from `net` and loads them into `target_net` before computing the targets. You can access the state of a networks weights using `net.state_dict()`. This is useful for saving files with your model weights (see [saving in pytorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)). Weights can be loaded into a model using `net.load_state_dict()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC3MmR4YVvuY"
   },
   "outputs": [],
   "source": [
    "def solve(net, target_net, loss_function, optimizer, discount, min_time=False):\n",
    "\n",
    "  if min_time:\n",
    "    G = G_min_time\n",
    "  else:\n",
    "    G = G_quadratic\n",
    "  \n",
    "  final_loss = 0.0\n",
    "  for epoch in range(2000):\n",
    "    net.zero_grad()\n",
    "\n",
    "    ### TODO ###\n",
    "    # Update the target network with the previous weights from net\n",
    "\n",
    "    ############\n",
    "\n",
    "    with torch.no_grad():\n",
    "      Jnext = target_net.forward(Xnext)\n",
    "      Jd, ind = torch.min(G + discount*Jnext, dim=1) # we discount the future cost-to-go\n",
    "    J = net.forward(X)\n",
    "    loss = loss_function(J, Jd)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 199 == 0:\n",
    "      print('[%d] loss: %.6f' % (epoch + 1, loss.item()))\n",
    "      plot_and_compare(net, timestep, min_time=min_time)\n",
    "\n",
    "  final_loss = loss.item()\n",
    "  return net, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE1bDAwmFJdD"
   },
   "source": [
    "## Training the Networks\n",
    "\n",
    "Now that we defined the training loop, we can train our models! Feel free to play with the hyperparameters and the seed. \n",
    "\n",
    "**For the autograder to work, change the two cells below back to their original state.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tK8lpObpVy3t"
   },
   "outputs": [],
   "source": [
    "# For the autograder to work, you do not need to modify this cell!\n",
    "set_seed(12345)\n",
    "net_quadratic = Net()\n",
    "target_net_quadratic = Net()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net_quadratic.parameters(), lr=0.01)\n",
    "gamma_quadratic = 0.95 # discount factor\n",
    "net_quadratic, final_loss_quadratic = solve(net_quadratic, target_net_quadratic, loss_function, optimizer, gamma_quadratic, min_time=False)\n",
    "print(\"Final loss: \", final_loss_quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icJyOekUkU0R"
   },
   "outputs": [],
   "source": [
    "# For the autograder to work, you do not need to modify this cell!\n",
    "set_seed(12345)\n",
    "net_min_time = Net()\n",
    "target_net_min_time = Net()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net_min_time.parameters(), lr=0.0015, momentum=0.9)\n",
    "gamma_min_time = 0.95 # discount factor\n",
    "net_min_time, final_loss_min_time = solve(net_min_time, target_net_min_time, loss_function, optimizer, gamma_min_time, min_time=True)\n",
    "print(\"Final loss: \", final_loss_min_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7xbR3EEOXlI"
   },
   "source": [
    "## Extracting the Policy\n",
    "\n",
    "Remember that the cost-to-go implicitly encodes our policy - if we can score every state, we should know exactly where we want to go! Now that we trained a neural network to approximate the cost-to-go $J(x)$, we should be able to use this approximation to extract a policy and control our system. In the cell below, your job is to fill in the code in the `DoCalcVectorOutput` method where we have marked `### TODO ###`. Update the value of `torque` with the best action given the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uERy256Z9Ajv"
   },
   "outputs": [],
   "source": [
    "class Policy(VectorSystem):\n",
    "\n",
    "    def __init__(self, net, gamma, us, min_time=False):\n",
    "        # 2 inputs: double integrator state [q, q_dot]\n",
    "        # 1 output: control torque [u]\n",
    "        VectorSystem.__init__(self, 2,  1)\n",
    "        self.net = net\n",
    "        self.gamma = gamma\n",
    "        self.us = us # torch tensor with all possible actions\n",
    "        if min_time:\n",
    "          is_close = lambda x: np.isclose(x, [0., 0.]).astype(float)\n",
    "          self.cost = lambda x, u: 0.0 if np.dot(is_close(x), is_close(x).T) == 2.0 else 1.0\n",
    "        else:\n",
    "          self.cost = lambda x, u: np.dot(x, x.T) + u**2\n",
    "        self.timestep=0.1\n",
    "\n",
    "    def DoCalcVectorOutput(self, context, state, unused, torque):\n",
    "        ### TODO ###\n",
    "        # Find the best torque\n",
    "        # Note: this method doesn't return anything, it just updates 'torque'\n",
    "        torque[:] = [0.0]\n",
    "        ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd1TfDuyPJnq"
   },
   "source": [
    "## Simulating the System\n",
    "\n",
    "In the cell below we've plugged our policy into a drake diagram. We simulate the minimum time cost policy, and the quadratic regulator cost policy. There is nothing to implement here, but this is another opportunity to familiarize yourselves with drake so definitely take a look!\n",
    "\n",
    "The basic workflow involves instantiating a drake `diagram`, and adding `systems` to it. Systems can be pre-defined in drake (like the visualizer we use below for the double integrator), or we can define them ourselves (like the double integrator plant we define as a custom `LinearSystem`, or the policy which we implemented as a `VectorSystem` above). Once systems are added, we can connect their inputs and outputs to other systems to dictate the flow of information within the diagram. Lastly, we pass our diagram to `simulator` which will simulate and visualize the evolution of our overall system in time. \n",
    "\n",
    "Feel free to play with the starting state, and the simulation duration.\n",
    "\n",
    "**For the autograder to work, change back to the original starting state ([3., 3.]) and the original simulation duration (25)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y6FYW3EfptL"
   },
   "outputs": [],
   "source": [
    "def simulate_policy(minimum_time=False):\n",
    "  # Feel free to try different starting states\n",
    "  start_state = [3., 3.] # set to [3, 3] for the autograder!\n",
    "\n",
    "  # Instantiate the policy system\n",
    "  if minimum_time:\n",
    "    net_min_time.eval()\n",
    "    policy = Policy(net_min_time, gamma_min_time, us, min_time=True)\n",
    "  else:\n",
    "    net_quadratic.eval()\n",
    "    policy = Policy(net_quadratic, gamma_quadratic, us, min_time=False)\n",
    "\n",
    "  # Wire up the drake diagram\n",
    "  double_integrator_plant = LinearSystem(A=np.mat('0 1; 0 0'),\n",
    "                                         B=np.mat('0; 1'),\n",
    "                                         C=np.eye(2),\n",
    "                                         D=np.zeros((2,1)))\n",
    "  builder = DiagramBuilder() # instantiate a diagram builder\n",
    "  plant = builder.AddSystem(double_integrator_plant) # add a sub-system to it\n",
    "  vi_policy = builder.AddSystem(policy)\n",
    "  builder.Connect(plant.get_output_port(0), vi_policy.get_input_port(0)) # connecting inputs/outputs of two sub-systems in our diagram\n",
    "  builder.Connect(vi_policy.get_output_port(0), plant.get_input_port(0))\n",
    "  visualizer = builder.AddSystem(DoubleIntegratorVisualizer(show=plt_is_interactive))\n",
    "  builder.Connect(plant.get_output_port(0), visualizer.get_input_port(0))\n",
    "  diagram = builder.Build() # finish building the diagram\n",
    "\n",
    "  # Simulate the system\n",
    "  simulator = Simulator(diagram)\n",
    "  simulator.get_mutable_context().SetContinuousState(start_state) # set the initial state\n",
    "  AdvanceToAndVisualize(simulator, visualizer, 25.)\n",
    "\n",
    "  return simulator.get_mutable_context().get_continuous_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G4acSVHj2g4"
   },
   "outputs": [],
   "source": [
    "# Simulate the minimum time policy\n",
    "final_state_min_time = simulate_policy(minimum_time=True)\n",
    "print(\"Final State: \", final_state_min_time[0], final_state_min_time[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH_wZGUhnpxr"
   },
   "outputs": [],
   "source": [
    "# Simulate the quadratic cost policy\n",
    "final_state_quadratic = simulate_policy(minimum_time=False)\n",
    "print(\"Final State: \", final_state_quadratic[0], final_state_quadratic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGpqwd5E7bcy"
   },
   "source": [
    "## Written Questions\n",
    "\n",
    "1) Work through the coding sections in this notebook.\n",
    "\n",
    "2) Although it gets close, why might the system not stabilize exactly at the origin?\n",
    "\n",
    "3) In the notebook we implemented \"fitted value iteration\".  In the questions below, you'll be asked to think about the differences between the \"graph search\" version of value iteration we saw at the start of chapter 7, and fitted value iteration. Answer the questions below with a brief explanation:\n",
    "\n",
    "*   Practically, can graph search value iteration perform directly in a continuous state space? Can fitted value iteration?\n",
    "*   Practically, can graph search value iteration perform directly with a continuous action space? Can fitted value iteration?\n",
    "\n",
    "4) For our implementation, we assume deterministic transitions between states. This means that given a particular state $x_t$ and action $u_t$, we will always end up in the same unique $x_{t+1}$. Mathematically:\n",
    "\n",
    "$$x_{t+1} = f(x_t, u_t)$$\n",
    "\n",
    "In real world scenarios we often have to deal with disturbances or dynamics not captured by our model, which can cause our state transitions to be stochastic. Mathematically:\n",
    "\n",
    "$$P(x_{t+1} | x_t, u_t) = f(x_t, u_t)$$\n",
    "\n",
    "If we had stochastic transitions, which steps of the notebook implementation would need to change, and how would they change? The possible steps to choose from are listed below - you can select multiple. When describing the change, a verbal description is fine you do not need to include any code:\n",
    "\n",
    "* Sampling the set of points <code>X</code> before performing the fit. Note that this does not include <code>Xnext</code>.\n",
    "* Generating the costs <code>G</code> before performing the fit. Assume we are using the quadratic cost.\n",
    "* Updating the cost function target in the training loop: <code>Jd, ind = torch.min(G + discount*Jnext, dim=1)</code>.\n",
    "* Choosing the best torque when simulating the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VejEvzHyHdQ-"
   },
   "source": [
    "## Autograding\n",
    "The autograder has two tests:\n",
    "\n",
    "1) Test that the final loss generated for both the quadratic cost and the minimum time cost is within some range.\n",
    "\n",
    "2) Test that the generated policies land the system within the right final state range after simulation. \n",
    "\n",
    "You can check your work by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DexQbX3eI55O"
   },
   "outputs": [],
   "source": [
    "from underactuated.exercises.dp.fitted_double_integrator.test_fitted_double_integrator import TestFittedDoubleIntegrator\n",
    "from underactuated.exercises.grader import Grader\n",
    "Grader.grade_output([TestFittedDoubleIntegrator], [locals()], 'results.json')\n",
    "Grader.print_test_results('results.json')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "fitted_double_integrator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}