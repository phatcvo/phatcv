{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kiuxBR04DcZ"
   },
   "source": [
    "# Dynamics Regression and Graphical Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpqbPGN731Z8"
   },
   "source": [
    "## Notebook Setup\n",
    "The following cell will checkout the underactuated repository, and set up the path (only if necessary).\n",
    "- On Google's Colaboratory, this **will take approximately two minutes** on the first time it runs (to provision the machine), but should only need to reinstall once every 12 hours.  Colab will ask you to \"Reset all runtimes\"; say no to save yourself the reinstall.\n",
    "- On Binder, the machines should already be provisioned by the time you can run this; it should return (almost) instantly.\n",
    "\n",
    "More details are available [here](http://underactuated.mit.edu/drake.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xvu5-Bm531Z-"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os, sys\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Install drake (and underactuated).\n",
    "if 'google.colab' in sys.modules and importlib.util.find_spec('underactuated') is None:\n",
    "    urlretrieve(f\"http://underactuated.csail.mit.edu/scripts/setup/setup_underactuated_colab.py\",\n",
    "                \"setup_underactuated_colab.py\")\n",
    "    from setup_underactuated_colab import setup_underactuated\n",
    "    setup_underactuated(underactuated_sha='560c2adace05eb20ebd78377582015d5b2d3859a', drake_version='0.25.0', drake_build='releases')\n",
    "\n",
    "server_args = []\n",
    "if 'google.colab' in sys.modules:\n",
    "  server_args = ['--ngrok_http_tunnel']\n",
    "\n",
    "# Setup matplotlib.\n",
    "from IPython import get_ipython\n",
    "if get_ipython() is not None: get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "\n",
    "# python libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# underactuated imports\n",
    "from underactuated import plot_2d_phase_portrait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fjIsN4431aA"
   },
   "source": [
    "## Problem Description\n",
    "In this problem you will implement a simple neural network in pytorch to model the dynamics of the damped pendulum seen in class from data. At the end of the notebook, you will perform a graphical analysis and answer a few questions about the system.\n",
    "\n",
    "**These are the main steps of the exercise:**\n",
    "1. Implement the network architecture in pytorch\n",
    "2. Perform a graphical analysis of the dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hNO98ajN_z1"
   },
   "source": [
    "## Sampling measurements\n",
    "In class, we derived the dynamics for the damped pendulum system analytically:\n",
    "\n",
    "> $b\\dot\\theta = u_0 - mgl\\sin\\theta$\n",
    "\n",
    "In this problem, we pretend we don't know the relationship $\\dot \\theta = f(\\theta)$ and would like to learn it from $(\\theta, \\dot \\theta)$ data measurements. That is, we'd like to train a neural network $NN(\\theta) \\approx f(\\theta) = \\dot \\theta$.\n",
    "\n",
    "In the cell below, we assume a pendulum with $l = 1$ and $m = 1$ for simplicity. Gravity is $g = 9.81$, and we assume 0 torque. We generate 10000 evenly spaced samples from the ground truth equations, and then sample 100 examples and add some random noise to \"simulate\" taking 100 measurements. The data we're trying to fit is plotted below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akkyz7AzRTta"
   },
   "source": [
    "### Pytorch tip\n",
    "\n",
    "One tip you'll find useful when debugging is to fix the random seed generators for your machine learning library (in our case pytorch), and potentially for numpy as well. This allows us to ensure that our neural network will be initialized with the same values each time we run our code, and it will allow us to see the effects of our changes and make things easier to debug. We can fix the random generator seed as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuUod6sRZG68"
   },
   "outputs": [],
   "source": [
    "np.random.seed(5) # you can pick any integer for different seeds\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# System parameters. Feel free to play with these, but use\n",
    "# the original set for the autograder!\n",
    "m = 1\n",
    "l = 1\n",
    "g = 9.81\n",
    "u = 0\n",
    "\n",
    "def generate_measurements():\n",
    "  # create ground truth data points\n",
    "  theta = torch.unsqueeze(torch.linspace(-2*np.pi, 2*np.pi, 10000), dim=1)\n",
    "\n",
    "  # subsample 100 ground truth data points and add noise\n",
    "  idxs = np.random.choice(10000, 100)\n",
    "  theta = theta[idxs]\n",
    "  theta_dot = u - m*g*l*torch.sin(theta) + 1.5*torch.rand(theta.size()) - 1.5*torch.rand(theta.size())\n",
    "\n",
    "  return theta, theta_dot\n",
    "\n",
    "# plot sampled measurements\n",
    "theta, theta_dot = generate_measurements()\n",
    "plt.scatter(theta, theta_dot, s=5)\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('theta_dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N_XNXq0QveP"
   },
   "source": [
    "## Defining our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz3sl9MCSCAd"
   },
   "source": [
    "We will now implement our neural network in pytorch. Pytorch allows us to define a model class as a subclass of `nn.Module` (checkout the [pytorch docs](https://pytorch.org/docs/stable/index.html) for more information). We define our network architecture at initialization, and then we define a `forward` method that defines our forward pass. This method takes in the inputs to our neural network, and returns the outputs.\n",
    "\n",
    "We've provided an example model that shows you how to stack two linear layers with a Leaky ReLU nonlinearity in between them. Your job is to define `self.model`.\n",
    "\n",
    "- Layer 1: linear layer with 1 input and 200 outputs\n",
    "- Leaky ReLU nonlinearity\n",
    "- Layer 2: linear layer with 200 inputs and 100 outputs\n",
    "- Leaky ReLU nonlinearity\n",
    "- Layer 3: linear layer with 100 inputs and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4didPA2EDhN"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "\n",
    "    # Example model\n",
    "    self.example_model = nn.Sequential(\n",
    "        torch.nn.Linear(1, 50),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(50, 1)\n",
    "    )\n",
    "\n",
    "    ####################################\n",
    "    # Define your model\n",
    "\n",
    "    self.model = nn.Sequential(\n",
    "        # YOUR CODE HERE\n",
    "    )\n",
    "\n",
    "    ####################################\n",
    "\n",
    "  def forward(self, theta):\n",
    "    theta_dot_hat = self.model(theta)\n",
    "    return theta_dot_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5soWOWETglM"
   },
   "source": [
    "## Training our Network\n",
    "\n",
    "Now we can instantiate a network, an optimizing algorithm that will update the weights of our network, and a loss function. We will be using a mean-squared error loss for this problem. We are going to use the following optimizer and loss function:\n",
    "\n",
    "- Optimizer: [Adam optimizer](https://pytorch.org/docs/stable/optim.html)\n",
    "- Loss function: [Mean-squared Error loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
    "\n",
    "In the cell below, your job is to instantiate a mean-squared error loss, and an Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMfELFEPVImq"
   },
   "source": [
    "We then iteratively update the parameters of our network. This is done by feeding in the theta values we measured, and computing a loss using the outputs (the networks \"guess\" for the theta dot outputs). If our predicted theta dots are far from the theta dots we measured, then our loss will be high and vice versa. We use the loss to call `loss.backward()`, pytorch runs [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) to update the network weights we passed into the Adam optimizer at initialization for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyiw_7dVyeUZ"
   },
   "source": [
    "### Pytorch tip\n",
    "\n",
    "Note that we zero out our gradients before we step through our optimizer at each iteration. We need to do this because pytorch accumulates gradients after each pass. This is used to train other types of networks like RNN's, but for our purposes here we want just the new gradients at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EufqWTI0FjYe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(5) # you can pick any integer for different seeds\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Feel free to play with these, but keep the original values for the autograder.\n",
    "iterations = 200 # number of optimization iterations\n",
    "learning_rate = 0.05 # you do not need to tune this!\n",
    "\n",
    "# Generate measurements\n",
    "theta, theta_dot = generate_measurements()\n",
    "\n",
    "# Initialize our network, optimizer, and loss function\n",
    "NN = Network()\n",
    "\n",
    "####################################\n",
    "# Instantiate a mean-squared error loss\n",
    "# and an Adam optimizer. You can access\n",
    "# your networks parameters with NN.parameters().\n",
    "# You will need to pass this in as an\n",
    "# argument when instantiating the optimizer.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "optimizer = None\n",
    "L = None\n",
    "\n",
    "####################################\n",
    "\n",
    "final_loss = 0.0\n",
    "if optimizer is not None and L is not None:\n",
    "  for t in range(iterations):\n",
    "\n",
    "    theta_dot_hat = NN(theta) # pass data through your neural net, and generate x_ddot predictions\n",
    "\n",
    "    loss = L(theta_dot_hat, theta_dot) # compare your predictions with the measured x_ddot values from your data set\n",
    "\n",
    "    print('Loss @ time {}: {}'.format(t, loss.item()))\n",
    "\n",
    "    optimizer.zero_grad() # clear gradients from the last iteration, before generating gradients for this iteration\n",
    "    loss.backward() # run backpropagation, and compute gradients\n",
    "    optimizer.step() # apply gradients to your network weights\n",
    "\n",
    "  final_loss = loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md4TtaoW1CyG"
   },
   "source": [
    "## Evaluating our Model\n",
    "\n",
    "Now that we've trained our network, let's look at how well we did. To do this, we will feed in a range of theta values and see how well our networks predictions line up with the ground truth dynamics of our system. We plot the original data in blue, and our neural networks fit in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kep9pA0Vsh8c"
   },
   "outputs": [],
   "source": [
    "if optimizer is not None and L is not None:\n",
    "  theta_eval = torch.unsqueeze(torch.linspace(-3*np.pi, 3*np.pi, 10000), dim=1)\n",
    "  theta_ground = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
    "  theta_dot_ground = u - m*g*l*np.sin(theta_ground)\n",
    "\n",
    "  # NOTE: some layers like batchnorm or dropout layers need to be turned\n",
    "  # off during evaluation and turned on during training. Calling model.eval()\n",
    "  # will do this for you. We do not use these types of layers here, but we\n",
    "  # call eval() here to demonstrate for potential future use. To go back to \"training\"\n",
    "  # mode, call model.train()\n",
    "  NN.eval()\n",
    "\n",
    "  # evaluate our network\n",
    "  theta_dot_eval = NN(theta_eval)\n",
    "\n",
    "  # plot our fit\n",
    "  plt.scatter(theta_ground, theta_dot_ground, s=6)\n",
    "  plt.plot(theta_eval, theta_dot_eval.data.numpy(), color='red', lw=1) # this is how to convert a pytorch tensor to a numpy array\n",
    "  plt.plot(np.linspace(-4*np.pi, 4*np.pi, 3), np.zeros(3), color='black', lw=1)\n",
    "  plt.xlim(-3*np.pi, 3*np.pi)\n",
    "  plt.xlabel('theta')\n",
    "  plt.ylabel('theta_dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44cLk3K6x4ZY"
   },
   "source": [
    "## Written questions\n",
    "\n",
    "Answer these written exercises in a pdf (preferably latex) and upload it gradescope.\n",
    "\n",
    "1) Within the range $\\theta \\in [-5.0, 5.0]$, which values of $(\\theta, \\dot \\theta)$ represent fixed points? Which fixed points are stable and which are unstable?\n",
    "\n",
    "2) What is one reason why our model might not fit the ground truth data exactly?\n",
    "\n",
    "3) Does our model give us a reasonable approximation of the dynamics for all $(\\theta, \\dot \\theta)$ pairs? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-T-a5gAh3OH"
   },
   "source": [
    "## How Will this Notebook Be Graded?\n",
    "If you are enrolled in the class, this notebook will be graded using [Gradescope](https://www.gradescope.com).\n",
    "We will send you the details of how to access the course page in Gradescope by email.\n",
    "\n",
    "We will replicate your work by running your notebook and checking that the final loss value is within a reasonable range.\n",
    "\n",
    "You will get full score if the following test succeeds:\n",
    "- Training runs and the final loss value is within a reasonable range of the value we are generating.\n",
    "\n",
    "This should hold if you define your network appropriately, and choose the right optimizer/loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgUjMUG_h3OI"
   },
   "outputs": [],
   "source": [
    "from underactuated.exercises.pend.learning_dynamics.test_learning_dynamics import TestLearningDynamics\n",
    "from underactuated.exercises.grader import Grader\n",
    "Grader.grade_output([TestLearningDynamics], [locals()], 'results.json')\n",
    "Grader.print_test_results('results.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "learning_dynamics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}