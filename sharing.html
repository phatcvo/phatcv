<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="image/favicon.ico"> 
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Sharing</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Explore:</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="sharing.html" class="current">âž¥Sharing</a></div>
<div class="menu-item"><a href="news.html">More&nbsp;info</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Sharing</h1>
</div>
<p><section class="card"></p>
	<h2>RL</h2>
<ul>
	<p><h3>1. Fundamentals of RL</h3></p>
	<a href="/Sharing/RL/11Bandits_and_Exploration_vs_Exploitation.html" target="_blank">1.1. Bandits and Exploration & Exploitation</a><br />
	<a href="/Sharing/RL/12Optimal_Policies_with_Dynamic_Programming.html" target="_blank">1.2. Optimal Policies with Dynamic Programming</a><br />
	<a href="/Sharing/RL/Course1_Learning_Objectives.pdf" target="_blank">Course 1 Learning Objectives</a><br />
</ul>
<ul>
	<p><h3>2. Sample based Learning Methods</h3></p>
	<a href="/Sharing/RL/21Policy_Evaluation_with_Temporal_Difference_Learning.html" target="_blank">2.1. Policy Evaluation with TD-Learning</a><br />
	<a href="/Sharing/RL/22Q-Learning_and_Expected_Sarsa.html" target="_blank">2.2. Q-Learning and Expected Sarsa</a><br />
	<a href="/Sharing/RL/23Dyna-Q _and_Dyna-Q+.html" target="_blank">2.3. Dyna-Q and Dyna-Q+</a><br />
	<a href="/Sharing/RL/Course2_Learning_Objectives.pdf" target="_blank">Course 2 Learning Objectives</a><br /><br />
</ul>
<ul>
	<p><h3>3. Prediction and Control with Function Approximation</h3></p>
	<a href="/Sharing/RL/" target="_blank">3.1. Semi-gradient TD(0) with State Aggregation</a><br />
	<a href="/Sharing/RL/" target="_blank">3.2. Semi-gradient TD with a Neural Network</a><br />
	<a href="/Sharing/RL/" target="_blank">3.3. Function Approximation and Control</a><br />
	<a href="/Sharing/RL/" target="_blank">3.4. Average Reward Softmax Actor-Critic</a><br />
	<a href="/Sharing/RL/" target="_blank">Course 3 Learning Objectives</a><br />
</ul>
<ul>
	<p><h3>4. A Complete Reinforcement Learning System</h3></p>
	<a href="/Sharing/RL/" target="_blank">4.1. MoonShot Technologies</a><br />
	<a href="/Sharing/RL/" target="_blank">4.2. Implement your agent</a><br />
	<a href="/Sharing/RL/" target="_blank">4.3. Completing the Parameter Study</a><br />
	<a href="/Sharing/RL/" target="_blank">Course 4 Learning Objectives</a><br />
</ul>
</section></p>
</td>
</tr>
</table>
</body>
</html>
